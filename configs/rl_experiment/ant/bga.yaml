method: bga  # Bayesian gradient ascent.

out_dir: './experiments/rl_experiments/ant/bga/'  # Directory for experiment results.

environment_name: Ant-v1
mlp:
    layers: [111, 8]
    add_bias: True
    state_normalization: True
    manipulate_reward: 
        shift: 1
        scale: 4000

trials: 3
# Either choose max_iterations or max_objective_calls unequal None.
max_iterations:
max_objective_calls: 75000

optimizer_config: 
    max_samples_per_iteration: 64
    OptimizerTorch: sgd
    optimizer_torch_config: 
        lr: .5
    lr_schedular:
    Model: derivative_gp
    model_config:
        prior_mean: 0.
        ard_num_dims: dim_search_space  # If not None, each input dimension gets its own separate lengthscale.
        N_max: 256
        lengthscale_constraint: 
            constraint:
            kwargs:
        lengthscale_hyperprior: 
            prior: uniform
            kwargs: 
                a: 0.01
                b: 0.5
        outputscale_constraint:
            constraint: greather_than
            kwargs: 
                lower_bound: 0.001
        outputscale_hyperprior:
            prior: normal
            kwargs: 
                loc: 1.
                scale: .5
        noise_constraint: 
            constraint:
            kwargs:
        noise_hyperprior:
            prior:
            kwargs:
    hyperparameter_config: 
        optimize_hyperparameters: True
        hypers:
            covar_module.base_kernel.lengthscale:
            covar_module.outputscale:
            likelihood.noise: .01
        no_noise_optimization: True
    optimize_acqf: bga
    optimize_acqf_config: 
        q: 1
        num_restarts: 5
        raw_samples: 64
    # Either choose bounds or delta unequal None.
    bounds: 
        lower_bound:
        upper_bound:
    delta: 0.1
    epsilon_diff_acq_value: 0.001
    generate_initial_data:
    standard_deviation_scaling: False
    normalize_gradient: True
    verbose: True
